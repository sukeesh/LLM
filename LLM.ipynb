{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27ad986f-846e-49ad-842b-1dda8d3f367b",
   "metadata": {},
   "source": [
    "## Multi-Head Self-Attention (With Causal Masking)\n",
    "\n",
    "It helps the model understand which words in a sentence are important to each other.\n",
    "\n",
    "### How Self-attention works?\n",
    "\n",
    "1. Each word in the sentence is turned into a number.\n",
    "2. Each word creates three special versions of itself:\n",
    "\n",
    "    a. Query (Q) -> \"What am I looking for?\"\n",
    "   \n",
    "    b. Key (K) -> \"What do I contain?\"\n",
    "   \n",
    "    c. Value (V) -> \"What information do I give?\"\n",
    "4. The model compares each word with every other word using dot product.\n",
    "    a. This tells the model how much attention one word should pay to another.\n",
    "5. The model applies a softmax function to make all attention values sum to 1.\n",
    "    a. This means each word gets a percentage of importance.\n",
    "6. The final output is a weighted sum of the Values (V) based on attention scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c244ec03-964c-4540-a7cb-76a5bdb4c4d0",
   "metadata": {},
   "source": [
    "## What is Multi-Head Attention?\n",
    "\n",
    "Instead of doing this once, we do it multiple times (heads) to capture different relationships.\n",
    "- One head may focus on subjects (cat -> sat).\n",
    "- Another head may focus on objects (sat -> mat).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29e4f43-b609-4686-b5d8-491af380ea8a",
   "metadata": {},
   "source": [
    "## What is Casual Masking?\n",
    "\n",
    "When generating text, we dont want a word to see future words. We use a mask to block attention to future words.\n",
    "\n",
    "For example, in:\n",
    "\"The cat sat...\"\n",
    "When generating sat, the model should only look at \"The cat\" and NOT at words ahead. So, we set future words to 0 in the attention matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f386c0-d594-4c0d-8e8a-07060a6a6d52",
   "metadata": {},
   "source": [
    "## Why is Self-Attention Important?\n",
    "\n",
    "- It helps model focus on relevant words while ignoring unrelated ones.\n",
    "- It understands word relationships in a sentence.\n",
    "- Unlike older models (like RNNs), it can look at the entire sentence at once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54bd68c-6a37-4e64-9789-7d163ccc033e",
   "metadata": {},
   "source": [
    "## Math of Multi Head Self Attention\n",
    "- Step 1: Convert words to vectors (embeddings)\n",
    "- Step 2: Create Query(Q), Key(K) and Value (V) Matrices.\n",
    "\n",
    "To get Q, K, and V, we multiply the embeddings by weight matrices:\n",
    "\n",
    "- Q = X . W<sub>Q</sub>\n",
    "- K = X . W<sub>K</sub>\n",
    "- V = X . W<sub>V</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ca7017-d681-460c-a7a5-302670f9aa5b",
   "metadata": {},
   "source": [
    "## Compute Attention Scores Using Scaled Dot-Product\n",
    "\n",
    "After getting Q and K, we compute the attention scores using the following dot product.\n",
    "\n",
    "Attention Score = Q . K<sup>T</sup>\n",
    "\n",
    "- The dot product b/w Q and K tells us how similar two words aer.\n",
    "- If two words are similar, their dot product is high and vice versa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ee7970-c7e4-4e9b-b4cb-62e6d26ed367",
   "metadata": {},
   "source": [
    "## Apply Scaling\n",
    "\n",
    "We divide above attention scores by √(d<sub>k</sub>)\n",
    "\n",
    "Scaled Score = QK<sup>T</sup>/√(d<sub>k</sub>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224b714f-bfc5-4535-ae58-256940b3acf9",
   "metadata": {},
   "source": [
    "## Apply Softmax\n",
    "\n",
    "After scaling, we apply the softmax function row-wise:\n",
    "\n",
    "Softmax(Scaled Scores)\n",
    "\n",
    "### Why Softmax?\n",
    "- Converts scores into probabilities\n",
    "- Each row sums to 1, meaning the each word \"distributes\" its attention properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0f90d6-4f70-43d1-ac28-06b2e033e0ee",
   "metadata": {},
   "source": [
    "### Compute Weighted Sum with V (Values)\n",
    "\n",
    "Output = Softmax Scores x V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab689a3e-73f2-43e4-8930-a83a2600b44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b1b7c75-5cef-4c94-be6b-c3168c3dc49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    d_k = Q.shape[-1]\n",
    "    attn_scores = torch.matmul(Q, K.transpose(-2, -1))\n",
    "    attn_scores = attn_scores / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "\n",
    "    if mask is not None:\n",
    "        attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "    attn_probs = F.softmax(attn_scores, dim=-1)\n",
    "    output = torch.matmul(attn_probs, V)\n",
    "    return output, attn_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ea16ee9-78bc-4839-95b7-778a9b567b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, embed_size, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = embed_size // num_heads  # Dimension per head\n",
    "        \n",
    "        # Linear layers to project input to Q, K, V\n",
    "        self.W_q = torch.nn.Linear(embed_size, embed_size)\n",
    "        self.W_k = torch.nn.Linear(embed_size, embed_size)\n",
    "        self.W_v = torch.nn.Linear(embed_size, embed_size)\n",
    "        \n",
    "        # Final linear layer after concatenation of heads\n",
    "        self.W_o = torch.nn.Linear(embed_size, embed_size)\n",
    "    \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        batch_size = Q.shape[0]\n",
    "        \n",
    "        # Apply linear transformations and split into multiple heads\n",
    "        Q = self.W_q(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Compute scaled dot-product attention\n",
    "        attention_output, attn_probs = scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # Concatenate heads and pass through final linear layer\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)\n",
    "        output = self.W_o(attention_output)\n",
    "        \n",
    "        return output, attn_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffea0ad8-13bf-40ed-b023-215bf39718ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Shape: torch.Size([1, 3, 8])\n",
      "Attention Weights: tensor([[[[0.3297, 0.3297, 0.3406],\n",
      "          [0.3209, 0.3352, 0.3439],\n",
      "          [0.3286, 0.3308, 0.3406]],\n",
      "\n",
      "         [[0.3601, 0.2791, 0.3608],\n",
      "          [0.3198, 0.3448, 0.3354],\n",
      "          [0.3501, 0.3008, 0.3492]]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Test Case\n",
    "batch_size = 1\n",
    "seq_len = 3\n",
    "embed_size = 8\n",
    "num_heads = 2\n",
    "\n",
    "torch.manual_seed(42)\n",
    "input_tensor = torch.rand(batch_size, seq_len, embed_size)\n",
    "\n",
    "# Create Multi-Head Attention Module\n",
    "multi_head_attn = MultiHeadAttention(embed_size, num_heads)\n",
    "\n",
    "# Forward Pass\n",
    "output, attn_probs = multi_head_attn(input_tensor, input_tensor, input_tensor)\n",
    "\n",
    "print(\"Output Shape:\", output.shape)  # Expected: (1, 3, 8)\n",
    "print(\"Attention Weights:\", attn_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42d9f133-0379-4a8a-b666-a1078e884972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Words: ['She', 'loves', 'cats']\n",
      "Output Shape: torch.Size([1, 3, 8])\n",
      "Attention Weights: tensor([[[[0.1895, 0.1881, 0.6225],\n",
      "          [0.2294, 0.3782, 0.3925],\n",
      "          [0.3702, 0.4393, 0.1906]],\n",
      "\n",
      "         [[0.4134, 0.3067, 0.2798],\n",
      "          [0.2474, 0.3777, 0.3750],\n",
      "          [0.3383, 0.3749, 0.2868]]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Test Case with Actual Words\n",
    "vocab = {\"she\": 0, \"loves\": 1, \"cats\": 2, \"dogs\": 3}\n",
    "embed_size = 8\n",
    "num_heads = 2\n",
    "seq_len = 3\n",
    "batch_size = 1\n",
    "\n",
    "# Define Embedding Layer\n",
    "torch.manual_seed(42)\n",
    "embedding_layer = torch.nn.Embedding(len(vocab), embed_size)\n",
    "\n",
    "# Sample sentence: \"She loves cats\"\n",
    "input_tokens = torch.tensor([[vocab[\"she\"], vocab[\"loves\"], vocab[\"cats\"]]])  # Shape: (1, 3)\n",
    "input_embeddings = embedding_layer(input_tokens)  # Convert tokens to embeddings\n",
    "\n",
    "# Create Multi-Head Attention Module\n",
    "multi_head_attn = MultiHeadAttention(embed_size, num_heads)\n",
    "\n",
    "# Forward Pass\n",
    "output, attn_probs = multi_head_attn(input_embeddings, input_embeddings, input_embeddings)\n",
    "\n",
    "print(\"Input Words: ['She', 'loves', 'cats']\")\n",
    "print(\"Output Shape:\", output.shape)  # Expected: (1, 3, 8)\n",
    "print(\"Attention Weights:\", attn_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad1c552-b4c8-49a9-9144-88788e12515c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sukeesh-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
